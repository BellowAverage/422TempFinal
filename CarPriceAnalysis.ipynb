{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'url', 'region', 'region_url', 'price', 'year', 'manufacturer',\n",
       "       'model', 'condition', 'cylinders', 'fuel', 'odometer', 'title_status',\n",
       "       'transmission', 'VIN', 'drive', 'size', 'type', 'paint_color',\n",
       "       'image_url', 'description', 'county', 'state', 'lat', 'long',\n",
       "       'posting_date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('vehicles.csv')\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning & Fitting Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression Model: Ridge Regression\n",
    "\n",
    "### Rationale of Selection\n",
    "\n",
    "- Ridge Regression adds an L2 penalty to the loss function. This shrinks coefficients but generally keeps all features in the model.\n",
    "- Lasso (L1 penalty) tends to zero out coefficients of less important features, effectively performing feature selection. This can be useful in some scenarios, but it can also drop too many correlated features and become unstable if you want to leverage those correlations.\n",
    "- In real-world used-car datasets, you may have correlated features (e.g., year and odometer, or manufacturer and model). Ridge handles multicollinearity more gracefully—rather than arbitrarily zeroing out correlated features, it shrinks them equally. This usually yields more stable results.\n",
    "- Elastic Net (a mix of L1 and L2) is also a possibility, but if you don’t specifically need feature selection from Lasso, Ridge is a solid, simpler choice.\n",
    "\n",
    "### Strengths\n",
    "- Very fast to train on large tabular data.\n",
    "- Interpretability: You can easily examine which features have higher coefficients.\n",
    "- Good baseline to quickly gauge your data’s linear separability.\n",
    "\n",
    "### Weaknesses\n",
    "- Struggles with non-linear relationships unless you manually engineer polynomial or interaction terms.\n",
    "- May still underfit if the real relationship between features and price is complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tree-Based Model: Random Forest Regressor\n",
    "\n",
    "### Rationale of Selection\n",
    "- Random Forest is an ensemble of many decision trees, each trained on different subsets of the data (rows and/or columns).\n",
    "- It naturally captures non-linearities and feature interactions without extensive feature engineering.\n",
    "- It is fairly robust to outliers and can handle missing values (with proper strategy or imputation).\n",
    "- Typically achieves strong performance on tabular data.\n",
    "\n",
    "### Strengths\n",
    "- High predictive power and often outperforms simple linear models.\n",
    "- Inherent feature importance assessment, which helps with interpretability.\n",
    "- Averaging across multiple trees reduces overfitting compared to a single decision tree.\n",
    "\n",
    "### Weaknesses\n",
    "- Can be memory-intensive if a large number of trees is used.\n",
    "- Predictions can be slower due to the need to average many decision trees.\n",
    "- Less straightforward to interpret compared to a pure linear model (though feature importances can mitigate that)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Network Model: Multilayer Perceptron (MLP)\n",
    "\n",
    "### Rationale of Selection\n",
    "- A feedforward MLP is generally the best neural network choice for tabular data (as opposed to CNNs for image data or LSTMs for sequence data).\n",
    "- Can capture complex non-linear relationships if given enough layers/neurons.\n",
    "- Offers flexibility with different architectures, regularization schemes (dropout, batch normalization), and activation functions.\n",
    "\n",
    "### Strengths\n",
    "- Highly flexible universal approximator; can learn intricate patterns in the data.\n",
    "- Scales well with more data, potentially outperforming simpler models given sufficient examples.\n",
    "- Possible to use embedding layers for high-cardinality categorical features, reducing dimensionality.\n",
    "\n",
    "### Weaknesses\n",
    "- Tuning hyperparameters (number of layers, neurons, learning rate, etc.) can be complex and time-consuming.\n",
    "- More prone to overfitting if not properly regularized.\n",
    "- Less interpretable compared to linear or tree-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Nearest Neighbor Model: K-Nearest Neighbors (KNN) Regressor\n",
    "\n",
    "### Rationale of Selection\n",
    "- **KNN Regressor** predicts a target value based on the average (or another aggregation) of the k nearest neighbors in feature space.\n",
    "- No explicit “training” phase—KNN stores the entire dataset and performs distance-based lookups during prediction.\n",
    "- Can handle complex boundaries since the decision (or prediction) is based purely on local neighbor information.\n",
    "\n",
    "### Strengths\n",
    "- Simple to implement and understand conceptually.\n",
    "- Naturally captures non-linear relationships if the data in the local neighborhood is consistent.\n",
    "- Few parameters: mainly k (number of neighbors) and distance metric (e.g., Euclidean).\n",
    "\n",
    "### Weaknesses\n",
    "- Can be **slow at prediction time** for large datasets because it must search through all or a large portion of the data.\n",
    "- **Sensitive to feature scaling**—proper normalization/standardization is crucial.\n",
    "- Choosing the optimal number of neighbors (k) can be non-trivial and must be tuned with techniques like cross-validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
